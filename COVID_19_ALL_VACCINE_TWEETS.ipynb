{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fb438e9f811a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "#https://www.kaggle.com/gpreda/all-covid19-vaccines-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"vaccination_all_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING DATA table\n",
    "\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum()\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
    "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    types = []\n",
    "    for col in data.columns:\n",
    "        dtype = str(data[col].dtype)\n",
    "        types.append(dtype)\n",
    "    tt['Types'] = types\n",
    "    return(np.transpose(tt))\n",
    "\n",
    "missing_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent values\n",
    "\n",
    "def most_frequent_values(data):\n",
    "    total = data.count()\n",
    "    tt = pd.DataFrame(total)\n",
    "    tt.columns = ['Total']\n",
    "    items = []\n",
    "    vals = []\n",
    "    for col in data.columns:\n",
    "        itm = data[col].value_counts().index[0]\n",
    "        val = data[col].value_counts().values[0]\n",
    "        items.append(itm)\n",
    "        vals.append(val)\n",
    "    tt['Most frequent item'] = items\n",
    "    tt['Frequence'] = vals\n",
    "    tt['Percent from total'] = np.round(vals / total * 100, 3)\n",
    "    return(np.transpose(tt))\n",
    "\n",
    "most_frequent_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations\n",
    "\n",
    "def plot_count(feature, title, df, size=1, ordered=True):\n",
    "    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n",
    "    total = float(len(df))\n",
    "    if ordered:\n",
    "        g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n",
    "    else:\n",
    "        g = sns.countplot(df[feature], palette='Set3')\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "    if(size > 2):\n",
    "        plt.xticks(rotation=90, size=8)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height,\n",
    "                '{:1.2f}%'.format(100*height/total),\n",
    "                ha=\"center\") \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-474aa638e484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#USER LOCATION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user_location\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"User location\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_count' is not defined"
     ]
    }
   ],
   "source": [
    "#USER LOCATION\n",
    "\n",
    "plot_count(\"user_location\", \"User location\", data,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORDCLOUDS\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "def show_wordcloud(data, title=\"\"):\n",
    "    text = \" \".join(t for t in data.dropna())\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"t\", \"co\", \"https\", \"amp\", \"U\"])\n",
    "    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,background_color=\"black\").generate(text)\n",
    "    fig = plt.figure(1, figsize=(16,16))\n",
    "    plt.axis('off')\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    fig.subplots_adjust(top=2.3)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOST PREVALENT WORDS IN TWEETS FROM THE USA\n",
    "\n",
    "us_df = data.loc[data.user_location==\"United States\"]\n",
    "show_wordcloud(us_df['text'], title = 'Prevalent words in tweets from US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def find_sentiment(post):\n",
    "    if sia.polarity_scores(post)[\"compound\"] > 0:\n",
    "        return \"Positive\"\n",
    "    elif sia.polarity_scores(post)[\"compound\"] < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment(df, feature, title):\n",
    "    counts = df[feature].value_counts()\n",
    "    percent = counts/sum(counts)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "    counts.plot(kind='bar', ax=ax1, color='green')\n",
    "    percent.plot(kind='bar', ax=ax2, color='blue')\n",
    "    ax1.set_ylabel(f'Counts : {title} sentiments', size=12)\n",
    "    ax2.set_ylabel(f'Percentage : {title} sentiments', size=12)\n",
    "    plt.suptitle(f\"Sentiment analysis: {title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['text'].apply(lambda x: find_sentiment(x))\n",
    "plot_sentiment(data, 'sentiment', 'Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['id', 'user_name', 'user_location', 'user_description', 'user_created',\n",
    "       'user_followers', 'user_friends', 'user_favourites', 'user_verified',\n",
    "       'date', 'hashtags', 'source', 'retweets', 'favorites',\n",
    "       'is_retweet', 'sentiment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stopword from nltk\n",
    "nltk.download('stopwords')\n",
    "stopword = set(stopwords.words('english'))\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Punkt Sentence Tokenizer\n",
    "\n",
    "This tokenizer divides a text into a list of sentences\n",
    "by using an unsupervised algorithm to build a model for abbreviation\n",
    "words, collocations, and words that start sentences.  It must be\n",
    "trained on a large collection of plaintext in the target language\n",
    "before it can be used.\n",
    "\n",
    "WordNet is used for lemmatizing \n",
    "'''\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tweets\n",
    "The Preprocessing steps taken are:\n",
    "\n",
    "Lower Casing: Each text is converted to lowercase.\n",
    "\n",
    "Removing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"\".\n",
    "\n",
    "Removing Usernames: Replace @Usernames with word \"\". (eg: \"@XYZ\" to \"\")\n",
    "\n",
    "Removing Short Words: Words with length less than 2 are removed.\n",
    "\n",
    "Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n",
    "\n",
    "Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: “wolves” to “wolf”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "\n",
    "def process_tweets(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    tweet=tweet[1:]\n",
    "    # Removing all URls \n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet) \n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #Removing Stop Words\n",
    "    final_tokens = [w for w in tokens if w not in stopword]\n",
    "    #reducing a word to its word stem \n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply process_tweets function to each entry in the text feature\n",
    "data['processed_tweets'] = data['text'].apply(lambda x: process_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def run_vader(text):\n",
    "    return sid.polarity_scores(text)['compound']\n",
    "\n",
    "data['vader_compound'] = data['processed_tweets'].apply(lambda x: run_vader(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive stats on vader_compound score column\n",
    "\n",
    "data['vader_compound'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a histogram to see distribution of vader_compound intensity scores\n",
    "data['vader_compound'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column called, \"polarity classification\"(binary either positive or negative polarity)\n",
    "\n",
    "'''\n",
    "rule:\n",
    "\n",
    "if vader_compound score >= 0.5: polarity = positive\n",
    "else if vader_compound score <= -0.5: polarity = negative\n",
    "'''\n",
    "\n",
    "def sentiment_classification(row):\n",
    "    classification = 100\n",
    "    if row > 0.05:\n",
    "        classification = 1 #positive\n",
    "    elif row < -0.5:\n",
    "        classification = 0\n",
    "    return classification\n",
    "\n",
    "#apply sentiment_classification function on each row in the vader_compound attribute\n",
    "\n",
    "data['polarity'] = data['vader_compound'].apply(lambda x: sentiment_classification(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove neutral scores -0.05 < vader_compound < 0.05\n",
    "\n",
    "data.drop(data.loc[data['polarity']==100].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset is not balanced, use random under-sampling to balance dataset\n",
    "#way more positives than negatives\n",
    "\n",
    "positive_count_1, negative_count_0 = data['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_1 = data[data['polarity'] == 1]\n",
    "negative_0 = data[data['polarity'] == 0]\n",
    "\n",
    "print('positive:', positive_1.shape)\n",
    "print('negative:', negative_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_1_under = positive_1.sample(negative_count_0)\n",
    "test_under = pd.concat([positive_1_under, negative_0], axis=0)\n",
    "\n",
    "print(\"total class of 1 and0:\",test_under['polarity'].value_counts())# plot the count after under-sampeling\n",
    "test_under['polarity'].value_counts().plot(kind='bar', title='count (target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_under.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_under.to_csv('balanced_twitter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
